# lightning.pytorch==2.5.0.post0
seed_everything: 42
trainer:
  accelerator: auto
  strategy: auto
  devices: auto
  num_nodes: 1
  precision: 32
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      name: giBERTino-GPT-small-GCN-molweni
      save_dir: lightning_logs
      project: giBERTino
      log_model: false
  callbacks:
    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        monitor: val/loss
        min_delta: 0.0
        patience: 10
        verbose: true
        mode: min
        strict: true
        check_finite: true
        log_rank_zero_only: false
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        filename: giBERTino-GPT-small-GCN-balanced-epoch-{epoch:02d}
        monitor: val/loss
        verbose: true
        save_top_k: 1
        save_weights_only: false
        mode: min
        auto_insert_metric_name: true
        save_on_train_epoch_end: true
        enable_version_counter: true
  fast_dev_run: false
  max_epochs: 30
  max_steps: -1
  overfit_batches: 0.0
  check_val_every_n_epoch: 1
  log_every_n_steps: 50
  accumulate_grad_batches: 1
  inference_mode: true
  use_distributed_sampler: true
  sync_batchnorm: false
  reload_dataloaders_every_n_epochs: 0
model:
  gnn_model: GCN
  in_channels: 41
  hidden_channels: 64
  num_layers: 32
  alpha: 0.5
  tokenizer: Alibaba-NLP/gte-modernbert-base
  bert_model: Alibaba-NLP/gte-modernbert-base
  lr: 1.0e-05
  relations: MOLWENI
  checkpoint_path: null
data:
  root: data/MOLWENI/alibaba-graphs
  dataset_name: MOLWENI
  batch_size: 32
  num_workers: 0
  negative_sampling_ratio: 50.0
  val_split_ratio: 0.1
  dataset_name: MOLWENI
optimizer: null
lr_scheduler: null
ckpt_path: null
